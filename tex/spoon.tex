% To do:
% ------
% - Don't forget to regularly pull and push with GitHub version.
% - Find out what MKN is writing and be complementary.
% - Write an outline.
% - Fill in the outline.
% - Publish.
% - Wait for the call from Stockholm.

% Style notes:
% ------------
% - Uhhh.

\documentclass[modern]{aastex631}
\usepackage[utf8]{inputenc}
\graphicspath{{./}{figures/}}

% Hogg typesetting issues
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-0.4in}
\setlength{\parindent}{1.2\baselineskip} % seriously
\frenchspacing\raggedbottom\sloppy\sloppypar

% Math issues
\newcommand{\teff}{T_{\mathrm{eff}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe}/\mathrm{H}]}

\shorttitle{machine learning and stellar parameters}
\shortauthors{apw \& dwh}

\begin{document}

\title{Data-driven models of, and with, the \textsl{Gaia} XP spectra}

\author{APW}
\author{DWH}
\author{others}

\begin{abstract}\noindent % seriously!
  Machine learning is having a huge impact in astrophysics.
  Nowhere is this more true than in the measurement of stellar parameters ($\teff$, $\logg$, $\feh$, and detailed abundances) from spectra (fluxes as a function of wavelength, or equivalents).
  In the language of regression, the stellar parameters are labels and the stellar spectra are features.
  Here we discuss some of the big successes in this space, and some of the important pitfalls and concerns.
  One success is that some data-driven methods are very fast, much faster than traditional physics-driven methods.
  Another success is that some data-driven methods appear to perform well at very low signal-to-noise.
  One pitfall is that many kinds of regressions effectively produce posterior inferences with the training set taking an unintended role as the prior pdf (or a sampling thereof).
  Another pitfall is that discriminative models sometimes learn more about correlations in the label space than they do relationships between features and labels.
  One concern is that machine-learning methods do not generally do well with missing data, realistic noise models, or respecting fundamental symmetries.
  In the long run, machine learning should be deployed to enhance human learning; we discuss the prospects for this possible future.
\end{abstract}

\section{Introduction} \label{sec:intro}

Hello World!

\section{Regression and models}

What is a regression?

What does an ML or stats person call a ``model''?

What is generative and what is discriminative?

Why is this latter distinction important?

What is known from a math / proof perspective, especially as regards risk and correlations in the label space?

\section{Data-driven models}

What is a data-driven model?

What assumptions do we make?

What assumptions and structures are possible?

\section{Discriminative models}

\section{Generative models}

\section{Physics-inspired or hybrid models}

\section{Symmetries, likelihoods, priors, and missing data}

HOGG: This section probably goes or modifies.

Imortant symmetries? One is the spectrograph resolution! Okay maybe it's not a symmetry.

ML cost functions are abominable.

ML regularizations are not priors.

How do various methods deal with missing data? Badly, if they are discriminative.

\section{Physics-aware methods; human learning}

There aren't any yet. But seriously; it isn't that hard. KORG?

How could we learn from what we are doing?

\section{Discussion}

Hello World!

\bibliography{sample631}{}
\bibliographystyle{aasjournal}

\end{document}
