% To do:
% ------
% - Don't forget to regularly pull and push with GitHub version.
% - Find out what MKN is writing and be complementary.
% - Write an outline.
% - Fill in the outline.
% - Publish.
% - Wait for the call from Stockholm.

% Style notes:
% ------------
% - Uhhh.

\documentclass[modern]{aastex631}
\usepackage[utf8]{inputenc}
\graphicspath{{./}{figures/}}
\usepackage{amsmath}

% Hogg typesetting issues
\addtolength{\textheight}{0.8in}
\addtolength{\topmargin}{-0.4in}
\setlength{\parindent}{1.2\baselineskip} % seriously
\frenchspacing\raggedbottom\sloppy\sloppypar

% Text issues
\newcommand{\acronym}[1]{\small{#1}}
\newcommand{\project}[1]{\textsl{#1}}
\newcommand{\Gaia}{\project{Gaia}}
\newcommand{\APOGEE}{\project{\acronym{APOGEE}}}

% Math issues
\newcommand{\true}[1]{\widetilde{#1}}
\newcommand{\given}{\,|\,}
\newcommand{\teff}{{T_{\mathrm{eff}}}}
\newcommand{\logg}{\log g}
\newcommand{\feh}{[\mathrm{Fe}/\mathrm{H}]}

\shorttitle{machine learning and stellar parameters}
\shortauthors{apw \& dwh}

\begin{document}

\title{Data-driven models of, and with, the \textsl{Gaia} XP spectra}

\author{APW}
\author{DWH}
\author{others}

\begin{abstract}\noindent % seriously!
  Machine learning and data-driven approaches are having a huge impact in astrophysics.
  Nowhere is this more true than in the measurement of stellar parameters ($\teff$, $\logg$, $\feh$, luminosities, and detailed abundances) from spectra (fluxes as a function of wavelength, or equivalents).
  Here we discuss approaches to obtain stellar parameters from the low-resolution ESA \Gaia\ XP spectra, which are delivered in terms of coefficients in a polynomial basis.
  We compare discriminative and generative models, and we also compare simple and complex models.
  One take-away is that generative models are far more capable when the test-set object has a spectrum with low signal-to-noise ratio.
  Another is\ldots something?
\end{abstract}

\section{Introduction} \label{sec:intro}

Hello World!

\section{Regression and models}

What is a regression?

What does an ML or stats person call a ``model''?

What is generative and what is discriminative?

Why is this latter distinction important?

What is known from a math / proof perspective, especially as regards risk and correlations in the label space?

\section{Data}

\Gaia\ XP DR3 data.... What do we get and what do we do with it?

\APOGEE\ DR17 data.... What do we get and what do we do with it?

\section{Problem set-up}

Each object $i$ has a spectrum, which is written as a $r$-dimensional vector $x_i$ of spectral values or real-valued numbers.
In the case of the \Gaia\ XP spectra, these will be the spectral coefficients or ratios of the spectral coefficients, or the spectral coefficients divided by some normalization. HOGG GET SPECIFIC HERE. In the language of regression, these $x_i$ are the \emph{features}

Each object $i$ also has a $q$-element list or $q$-dimensional vector $y_i$ of \emph{labels}, which are also (usually) real-valued numbers.
In the case of stellar-parameter label-transfer problems from \APOGEE, these might be $\teff$, $\logg$, $feh$.
In the case of luminosity or distance estimation, these might be \Gaia-measured absolute magnitudes $M_G$.

These measurements---both the features and the labels---come with uncertainties or noise estimates.
For our purposes, we will assume that the noise process is zero mean and approximately Gaussian, and we will assume that the Gaussian noise variances are known (delivered by the \Gaia\ pipelines, and correct).
That is, we will assume that
\begin{align}
  p(x_i\given\true{x_i}) &= N(x_i\given\true{x_i},C_{xi}) \\
  p(y_i\given\true{y_i}) &= N(y_i\given\true{y_i},C_{yi}) ~,
\end{align}
where $\true{x_i}, \true{y_i}$ are the ``true'' (unobserved, latent) values of $x_i, y_i$,
$N(x\given\mu,V)$ is the multivariate Gaussian pdf for a vector $x$ given vector mean $\mu$ and variance tensor $V$,
$C_{xi}$ is the known $r\times r$ variance tensor on the noise in the features,
and $C_{yi}$ is the known $q\times q$ variance tensor on the noise in the labels.

HOGG: How many training-set objects are there? Perhaps $n$? There is no ``test set''; there are just test objects.
The feature $r$-vectors $x_i$ and the label $q$-vectors $y_i$ can be usefully packed into rectangular arrays $X, Y$ such that
\begin{align}
  X^\top &= \begin{bmatrix}x_1 & x_2 & x_3 & \cdots & x_n\end{bmatrix} \\
  Y^\top &= \begin{bmatrix}y_1 & y_2 & y_3 & \cdots & y_n\end{bmatrix} ~,
\end{align}
where the transposes are shown to indicate that $X$ is $n\times r$ and $Y$ is $n\times q$.

\section{Discriminative models}

Linear regression.

What are regularizations?

Under- and over-parameterized models.

Linear regression in neighborhoods.

Deep learning (and the like).

Problems with missing features, and noisy features. SHOW that the model degrades terribly when the features get noisy.

Problems that most discriminative models are like posterior modes where the prior is the training set!

Problem that most methods that produce ``error bars'' have nothing to do with the true uncertainties.

\section{Generative models}

Discriminative models effectively assume that the features $x_i$ are very precisely (and accurately) known, such that it makes sense to predict the labels $y_i$ with a fixed arithmetic function of those features.
This isn't tenable when different stars have different noise and some have missing data.
That is, in practice---and especially with the \Gaia\ XP spectra---the features $x_i$ are noisily and heterogeneously measured.
A more accurate statement of our beliefs about the stars is that each star $i$ has a set of physical properties, which are \emph{latent properties}, not directly observed.
These latent properties produce or \emph{generate} both the labels $y_i$ \emph{and} the features $x_i$.
If those latent properties could be inferred or known, the labels (including missing labels) could be predicted, as could the features (including missing features).

Most physics-based models are generative models:
The star's latent properties the inputs to the simulation of the star; the functions that set the observables are the solutions to physical equations.
In data-driven contexts, generative models are built by simultaneously learning both the latent properties of each star, and also the functions that take the latent properties into the observables (which are the features and labels for the training set, and the features for the test objects).

The simplest generative model of this kind is the \emph{linear latent-variable model}.
In this model, both the features $x_i$ and the labels $y_i$ for object $i$ are generated by a $d$-dimensional latent variable $z_i$, which consists of $d$ real numbers:
\begin{align}
  x_i &= \mu_x + A\,z_i + \mbox{noise} \\
  y_i &= \mu_y + B\,z_i + \mbox{noise} ~,
\end{align}
where $\mu_x$ and $\mu_y$ are mean values for the $r$-vector features and the $q$-vector labels,
$A$ and $B$ are $r\times d$ and $q\times d$ rectangular matrices of coefficients,
and the ``noise'' is the residual.
If we think of the rectangular matrices $A, B$ as being themselves being made up of $d$-vectors, then
\begin{align}
  A^\top &= \begin{bmatrix}a_1 & a_2 & a_3 & \cdots & a_r\end{bmatrix} \\
  B^\top &= \begin{bmatrix}b_1 & b_2 & b_3 & \cdots & b_q\end{bmatrix} ~,
\end{align}
where each $d$-vector $a_j$ or $b_\ell$ is the vector whose inner product $a_j^\top z_i$ predicts (say) the $j$th component of the feature vector $x_i$ for object $i$.
Given these definitions, we can say
\begin{align}
  x_j &= [\mu_x]_j + Z\,a_j + \mbox{noise} \\
  y_\ell &= [\mu_y]_\ell + Z\,b_\ell + \mbox{noise} ~,
\end{align}
where now $x_j$ is the $j$th column of the data matrix $X$ (whereas $x_i$ was the $i$th row of $X$),
$y_\ell$ is the $\ell$th column of $Y$ (whereas $y_i$ was the $i$th row of $Y$),
$[\mu_x]_j$ is the $j$th component of the $r$-vector mean $\mu_x$,
and $[\mu_y]_\ell$ is the $\ell$th component of the $q$-vector mean $\mu_y$.
HOGG SAY: I don't love this notation. Maybe switch to $[X]_i$ and $[X^\top]_j$ and etc?

Up to this point, the model for the features $x_i$ looks a lot like PPCA or any other standard matrix factorization (HOGG CITE THINGS).
However, the key idea of this generative model is that the latent $d$-vectors $z_i$ simultaneously contain useful information about not just the spectra but also the stellar parameters, which are among the labels $z_i$.
Thus it is useful to hard-set part of the rectangular matrix $B$ such that there are direct and simple relationships between the components of the latent $z_i$ vectors and the labels $y_i$ we care about.
For example, if our labels are
\begin{align}
  y_i^\top &= \begin{bmatrix}\teff_i & \logg_i & \feh_i\end{bmatrix} ~,
\end{align}
such that $q=3$, and (say) $d=5$, we can hard-set the rectangular matrix $B$ to
\begin{align}
  B &= \begin{bmatrix}
    1 & 0 & 0 & 0 & 0 \\
    0 & 1 & 0 & 0 & 0 \\
    0 & 0 & 1 & 0 & 0
    \end{bmatrix} ~,
\end{align}
to ensure that the first three (of five) components of the latent vectors are the predictors for the three stellar parameters $\teff, \logg, \feh$.

  ... HOGG....

Under the joint assumptions that the model is reasonable, the stars are independent, the noise is Gaussian, and the noise variances are known, the likelihood of this model is given by
\begin{align}
  p(X, Y\given \mu_x, A, \mu_y, B, Z) &= \prod_i p(x_i\given \mu_x, A, z_i)\,p(y_i\given \mu_y, B, z_i) \\
  p(x_i\given \mu_x, A, z_i) &= N(x_i\given \mu_x + A\,z_i, C_{xi}) \\
  p(y_i\given \mu_y, B, z_i) &= N(y_i\given \mu_y + B\,z_i, C_{yi}) ~,
\end{align}
where $Z$ is the $n\times d$ rectangular block of all $d$-vector latent variables $z_i$,
and $N(x\given\mu,V)$ is the multivariate Gaussian pdf for a vector $x$ given vector mean $\mu$ and variance tensor $V$.

At fixed $A, B$, the $z_i$ vectors can be found by weighted least squares; this problem is linear.
At fixed $Z$, $A$ and the unknown parts of $B$ (the parts not set by definition) can be found by weighted least squares; this problem is \emph{also} linear.
That is, this entire linear latent-variable model can be optimized by iterated least squares, in which there is a \emph{$z$-step} in which $A, B$ are held fixed and $Z$ is optimized, followed by a \emph{$A$-step} in which $Z$ is held fixed and $A$ is optimized (plus any components of $B$ that are not set by definition).
The $z$-step can be simplified computationally by splitting the optimization of $Z$ into small optimizations by splitting the training set into objects (which are independent, by assumption), and the $A$-step can be simplified computationally similarly by splitting the optimization of $A$ into feature components (which are not independent, so WHAT HAPPENS?).

... HOGG: Pathology that the model can explain the spectra and labels independently. Want them jointly explained! Use regularization / priors to effect this joining. In particular, we will regularize how?

.... HOGG: Choose regularization strengths how?

... HOGG: ...

\section{Unsupervised models; clustering and de-noising}

HOGG: Start with assumptions.

HOGG: You can see the generative model above as a baby step towards a truly unsupervised model. Science is a set of unsupervised models, in the end!

HOGG: ...

\section{Physics-inspired or hybrid models}

We actually know a huge amount about how stars and stellar photospheres work.
So we should use this knowledge!
What would such a model look like?
It would be a model of the residuals, but with control parameters that are tied together between the data-driven residual model and the theory-driven baseline model.

There are also just very general physics principles, such as
spectrograph resolution, atomic line idenfication, and so on, that
suggest data representations that should simplify and improve
data-driven models. LIKE WHAT??

\section{Uncertainty propagation and estimation}

In general, discriminative models are worse than generative for accounting for uncertainties. WHY??

Most machine-learning methods that claim to deliver uncertainties are not truly delivering uncertainties. WHY?? Because you can't marginalize out the interior of the model; it's huge, and barely determined (there are large, exact degeneracies, and combinatorically large numbers of them).
Name and shame some of these methods!

\section{Discussion}

Hello World.

\bibliography{sample631}{}
\bibliographystyle{aasjournal}

\end{document}
